{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Recurrent Neural Networks for Nonlinear Dynamics Approximation\n",
                "## Application to the Duffing Oscillator\n",
                "\n",
                "---\n",
                "\n",
                "## Introduction\n",
                "\n",
                "### What are Recurrent Neural Networks (RNNs)?\n",
                "\n",
                "**Recurrent Neural Networks (RNNs)** are a class of artificial neural networks designed to handle **sequential data**. Unlike feedforward networks, RNNs have connections that loop back on themselves, allowing them to maintain a **hidden state** that captures information from previous time steps.\n",
                "\n",
                "The key equation for a simple (Elman) RNN is:\n",
                "\n",
                "$$h_t = \\tanh(W_{ih} x_t + W_{hh} h_{t-1} + b)$$\n",
                "\n",
                "Where:\n",
                "- $x_t$ is the input at time $t$\n",
                "- $h_t$ is the hidden state at time $t$\n",
                "- $W_{ih}$ and $W_{hh}$ are learnable weight matrices\n",
                "- $b$ is the bias term\n",
                "\n",
                "This recurrent structure allows RNNs to:\n",
                "- **Remember past information** through the hidden state\n",
                "- **Process variable-length sequences**\n",
                "- **Learn temporal dependencies** in data\n",
                "\n",
                "### Why use RNNs for Physics?\n",
                "\n",
                "Physical systems evolve over time according to differential equations. An RNN can learn to approximate these dynamics by:\n",
                "1. Taking a sequence of past inputs (e.g., forces applied to a system)\n",
                "2. Predicting the next state of the system\n",
                "\n",
                "This is particularly useful when:\n",
                "- The underlying equations are complex or unknown\n",
                "- Real-time predictions are needed\n",
                "- We want to create fast surrogate models\n",
                "\n",
                "---\n",
                "\n",
                "### What is an Implicit Scheme?\n",
                "\n",
                "When solving differential equations numerically, we have two main approaches:\n",
                "\n",
                "**Explicit Euler (Forward Euler):**\n",
                "$$y_{n+1} = y_n + \\Delta t \\cdot f(t_n, y_n)$$\n",
                "\n",
                "- Uses known values at time $t_n$ to compute $y_{n+1}$\n",
                "- Simple but can be **unstable** for stiff systems\n",
                "\n",
                "**Implicit Euler (Backward Euler):**\n",
                "$$y_{n+1} = y_n + \\Delta t \\cdot f(t_{n+1}, y_{n+1})$$\n",
                "\n",
                "- Uses the unknown value $y_{n+1}$ on both sides\n",
                "- Requires solving a (possibly nonlinear) equation at each step\n",
                "- Much more **stable**, especially for stiff problems\n",
                "\n",
                "For nonlinear problems like the Duffing oscillator, we use **Newton-Raphson iteration** to solve the implicit equation at each time step.\n",
                "\n",
                "---\n",
                "\n",
                "### The Duffing Oscillator\n",
                "\n",
                "The Duffing oscillator is a classic example of a **nonlinear dynamical system**:\n",
                "\n",
                "$$m\\ddot{x} + c\\dot{x} + k_1 x + k_3 x^3 = F(t)$$\n",
                "\n",
                "Where:\n",
                "- $m$ = mass\n",
                "- $c$ = damping coefficient  \n",
                "- $k_1$ = linear stiffness\n",
                "- $k_3$ = cubic stiffness (nonlinear term)\n",
                "- $F(t)$ = external forcing\n",
                "\n",
                "The cubic term $k_3 x^3$ makes this system exhibit rich nonlinear behavior including:\n",
                "- Multiple equilibrium points\n",
                "- Amplitude-dependent frequency\n",
                "- Potential for chaos (with certain parameters)\n",
                "\n",
                "---\n",
                "\n",
                "## Notebook Overview\n",
                "\n",
                "This notebook is divided into 4 parts:\n",
                "1. **Dataset Generation** - Simulate the Duffing oscillator using implicit Euler\n",
                "2. **RNN Training** - Train an RNN to predict the system's response\n",
                "3. **Model Evaluation** - Assess the model's performance on test data\n",
                "4. **Extrapolation Testing** - Test the model's ability to generalize"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy.optimize import fsolve\n",
                "from typing import Tuple, Optional\n",
                "import os\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "np.random.seed(42)\n",
                "torch.manual_seed(42)\n",
                "\n",
                "# Check device\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration\n",
                "\n",
                "We centralize all hyperparameters in a configuration class for easy modification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Config:\n",
                "    \"\"\"Centralized configuration for all hyperparameters\"\"\"\n",
                "    \n",
                "    # ===== Physical parameters of the Duffing system =====\n",
                "    m: float = 1.0      # Mass\n",
                "    c: float = 0.5      # Damping coefficient\n",
                "    k1: float = 1.0     # Linear stiffness\n",
                "    k3: float = 5.0     # Cubic stiffness (nonlinearity)\n",
                "    \n",
                "    # ===== Simulation parameters (training) =====\n",
                "    dt: float = 0.05           # Time step\n",
                "    t_max: float = 100.0       # Total simulation time\n",
                "    \n",
                "    # ===== Simulation parameters (extrapolation) =====\n",
                "    t_max_extrapolation: float = 200.0  # Time for extrapolation test\n",
                "    \n",
                "    # ===== Dataset parameters =====\n",
                "    window_size: int = 20      # Sliding window size (sequence length)\n",
                "    train_ratio: float = 0.7   # Training ratio\n",
                "    val_ratio: float = 0.15    # Validation ratio\n",
                "    \n",
                "    # ===== RNN model parameters =====\n",
                "    input_size: int = 1        # Input dimension (force at each timestep)\n",
                "    hidden_size: int = 64      # Hidden layer size\n",
                "    num_layers: int = 2        # Number of stacked RNN layers\n",
                "    dropout: float = 0.1       # Dropout rate\n",
                "    \n",
                "    # ===== Training parameters =====\n",
                "    batch_size: int = 32\n",
                "    learning_rate: float = 0.001\n",
                "    max_epochs: int = 1000\n",
                "    patience: int = 50         # Early stopping patience\n",
                "    min_delta: float = 1e-6    # Minimum improvement for early stopping\n",
                "    grad_clip: float = 1.0     # Gradient clipping threshold\n",
                "    \n",
                "    # ===== Paths =====\n",
                "    checkpoint_path: str = \"best_rnn_model.pt\"\n",
                "    \n",
                "    # ===== Device =====\n",
                "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "\n",
                "config = Config()\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"Configuration Summary\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Physical: m={config.m}, c={config.c}, k1={config.k1}, k3={config.k3}\")\n",
                "print(f\"Simulation: dt={config.dt}, t_max={config.t_max}\")\n",
                "print(f\"RNN: hidden_size={config.hidden_size}, num_layers={config.num_layers}\")\n",
                "print(f\"Training: batch_size={config.batch_size}, lr={config.learning_rate}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 1: Dataset Generation\n",
                "\n",
                "In this section, we:\n",
                "1. Define the forcing function\n",
                "2. Implement the implicit Euler solver\n",
                "3. Simulate the Duffing oscillator\n",
                "4. Create sliding window sequences for RNN training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.1 Force Generation\n",
                "\n",
                "We use a sum of sinusoids as the external forcing function. This creates a rich, time-varying input signal."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_force(t: np.ndarray, mode: str = \"standard\") -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Generate a time-varying force (sum of sinusoids).\n",
                "    \n",
                "    Args:\n",
                "        t: Time vector\n",
                "        mode: \"standard\" for training, \"extrapolation\" for testing\n",
                "        \n",
                "    Returns:\n",
                "        Force values over time\n",
                "    \"\"\"\n",
                "    if mode == \"standard\":\n",
                "        # Force used for training\n",
                "        return 2.0 * np.sin(0.5 * t) + 1.5 * np.cos(1.2 * t)\n",
                "    elif mode == \"extrapolation\":\n",
                "        # Different force for testing extrapolation\n",
                "        return 2.5 * np.sin(0.3 * t) + 2.0 * np.cos(0.8 * t) + 0.5 * np.sin(2.0 * t)\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown mode: {mode}\")\n",
                "\n",
                "\n",
                "# Visualize the force function\n",
                "t_demo = np.linspace(0, 20, 500)\n",
                "force_demo = generate_force(t_demo, \"standard\")\n",
                "\n",
                "plt.figure(figsize=(12, 4))\n",
                "plt.plot(t_demo, force_demo, 'b-', linewidth=1.5)\n",
                "plt.title(\"External Forcing Function F(t)\")\n",
                "plt.xlabel(\"Time (s)\")\n",
                "plt.ylabel(\"Force\")\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 Implicit Euler Solver\n",
                "\n",
                "We implement the **implicit Euler method** to solve the Duffing equation. Since the equation is nonlinear, we need to use **Newton-Raphson iteration** at each time step.\n",
                "\n",
                "The Duffing system can be written as two first-order ODEs:\n",
                "\n",
                "$$\\dot{x} = v$$\n",
                "$$\\dot{v} = \\frac{1}{m}(F(t) - cv - k_1 x - k_3 x^3)$$\n",
                "\n",
                "The implicit Euler discretization gives:\n",
                "\n",
                "$$x_{n+1} = x_n + \\Delta t \\cdot v_{n+1}$$\n",
                "$$v_{n+1} = v_n + \\frac{\\Delta t}{m}(F_{n+1} - c \\cdot v_{n+1} - k_1 x_{n+1} - k_3 x_{n+1}^3)$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def implicit_euler_step(x_n: float, v_n: float, f_n1: float, dt: float) -> Tuple[float, float]:\n",
                "    \"\"\"\n",
                "    Solve one time step using Implicit Euler + Newton-Raphson.\n",
                "    \n",
                "    The implicit Euler equations form a nonlinear system that we solve\n",
                "    using scipy's fsolve (which uses Newton-Raphson under the hood).\n",
                "    \n",
                "    Args:\n",
                "        x_n: Position at time n\n",
                "        v_n: Velocity at time n\n",
                "        f_n1: Force at time n+1\n",
                "        dt: Time step\n",
                "        \n",
                "    Returns:\n",
                "        Tuple (position, velocity) at time n+1\n",
                "    \"\"\"\n",
                "    def equations(p):\n",
                "        \"\"\"System of equations to solve for (x_{n+1}, v_{n+1})\"\"\"\n",
                "        x_new, v_new = p\n",
                "        \n",
                "        # Equation 1: x_{n+1} = x_n + dt * v_{n+1}\n",
                "        res1 = x_new - x_n - dt * v_new\n",
                "        \n",
                "        # Equation 2: v_{n+1} = v_n + (dt/m) * (F - c*v - k1*x - k3*x^3)\n",
                "        res2 = v_new - v_n - (dt / config.m) * (\n",
                "            f_n1 - config.c * v_new - config.k1 * x_new - config.k3 * x_new**3\n",
                "        )\n",
                "        return [res1, res2]\n",
                "    \n",
                "    # Solve using Newton-Raphson (initial guess: current state)\n",
                "    x_next, v_next = fsolve(equations, [x_n, v_n])\n",
                "    return x_next, v_next"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.3 Full Simulation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def simulate_duffing(\n",
                "    t_max: float, \n",
                "    dt: float, \n",
                "    force_mode: str = \"standard\"\n",
                ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
                "    \"\"\"\n",
                "    Simulate the Duffing oscillator for the specified duration.\n",
                "    \n",
                "    Args:\n",
                "        t_max: Total simulation time\n",
                "        dt: Time step\n",
                "        force_mode: Mode for force generation\n",
                "        \n",
                "    Returns:\n",
                "        Tuple (time, positions, velocities, forces)\n",
                "    \"\"\"\n",
                "    n_steps = int(t_max / dt)\n",
                "    t_eval = np.linspace(0, t_max, n_steps)\n",
                "    \n",
                "    # Initialize arrays\n",
                "    x_sim = np.zeros(n_steps)\n",
                "    v_sim = np.zeros(n_steps)\n",
                "    forces = generate_force(t_eval, mode=force_mode)\n",
                "    \n",
                "    print(f\"Simulating Duffing oscillator: {n_steps} time steps, t_max={t_max}s\")\n",
                "    \n",
                "    # Time-stepping loop\n",
                "    for i in range(n_steps - 1):\n",
                "        x_sim[i + 1], v_sim[i + 1] = implicit_euler_step(\n",
                "            x_sim[i], v_sim[i], forces[i + 1], dt\n",
                "        )\n",
                "    \n",
                "    return t_eval, x_sim, v_sim, forces\n",
                "\n",
                "\n",
                "# Run simulation\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"PART 1: DATASET GENERATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "t_eval, x_sim, v_sim, forces = simulate_duffing(config.t_max, config.dt, \"standard\")\n",
                "print(f\"Simulation complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the simulation results\n",
                "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
                "\n",
                "# Position plot\n",
                "axes[0].plot(t_eval, x_sim, 'b-', linewidth=1.5)\n",
                "axes[0].set_ylabel('Position x(t)', fontsize=12)\n",
                "axes[0].set_title('Duffing Oscillator Simulation (Implicit Euler)', fontsize=14)\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Force plot\n",
                "axes[1].plot(t_eval, forces, 'r-', linewidth=1.5)\n",
                "axes[1].set_xlabel('Time (s)', fontsize=12)\n",
                "axes[1].set_ylabel('Force F(t)', fontsize=12)\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.4 Create Sliding Window Sequences\n",
                "\n",
                "To train the RNN, we create sequences using a **sliding window** approach:\n",
                "- **Input (X)**: A window of past force values $[F_{t-W}, ..., F_{t-1}]$\n",
                "- **Output (y)**: The position at the next time step $x_t$\n",
                "\n",
                "This teaches the RNN to predict the system's response given a history of applied forces."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_sequences(data: np.ndarray, force: np.ndarray, window: int) -> Tuple[np.ndarray, np.ndarray]:\n",
                "    \"\"\"\n",
                "    Create sequences for RNN training using sliding window.\n",
                "    \n",
                "    Args:\n",
                "        data: Position data (what we want to predict)\n",
                "        force: Force data (input features)\n",
                "        window: Window size (sequence length)\n",
                "        \n",
                "    Returns:\n",
                "        Tuple (X, y) for training\n",
                "    \"\"\"\n",
                "    X, y = [], []\n",
                "    for i in range(len(data) - window):\n",
                "        # Input: window of force values\n",
                "        X.append(force[i:i + window])\n",
                "        # Output: position after the window\n",
                "        y.append(data[i + window])\n",
                "    return np.array(X), np.array(y)\n",
                "\n",
                "\n",
                "# Create sequences\n",
                "X, Y = create_sequences(x_sim, forces, config.window_size)\n",
                "print(f\"Sequences created: X.shape={X.shape}, Y.shape={Y.shape}\")\n",
                "print(f\"Each input sequence contains {config.window_size} force values\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.5 Create DataLoaders\n",
                "\n",
                "We split the data into:\n",
                "- **Training set (70%)**: Used to train the model\n",
                "- **Validation set (15%)**: Used for early stopping\n",
                "- **Test set (15%)**: Used for final evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_dataset() -> Tuple[DataLoader, DataLoader, DataLoader, dict]:\n",
                "    \"\"\"\n",
                "    Generate the complete dataset with train/val/test splits.\n",
                "    \n",
                "    Returns:\n",
                "        Tuple (train_loader, val_loader, test_loader, info_dict)\n",
                "    \"\"\"\n",
                "    # Simulation\n",
                "    t_eval, x_sim, v_sim, forces = simulate_duffing(config.t_max, config.dt, \"standard\")\n",
                "    \n",
                "    # Create sequences\n",
                "    X, Y = create_sequences(x_sim, forces, config.window_size)\n",
                "    print(f\"Sequences created: X.shape={X.shape}, Y.shape={Y.shape}\")\n",
                "    \n",
                "    # Convert to tensors\n",
                "    # Shape: [n_samples, window_size, 1] for X\n",
                "    # Shape: [n_samples, 1] for Y\n",
                "    X_t = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)\n",
                "    Y_t = torch.tensor(Y, dtype=torch.float32).unsqueeze(-1)\n",
                "    \n",
                "    # Split train/val/test\n",
                "    n_samples = len(X_t)\n",
                "    n_train = int(config.train_ratio * n_samples)\n",
                "    n_val = int(config.val_ratio * n_samples)\n",
                "    \n",
                "    X_train, Y_train = X_t[:n_train], Y_t[:n_train]\n",
                "    X_val, Y_val = X_t[n_train:n_train + n_val], Y_t[n_train:n_train + n_val]\n",
                "    X_test, Y_test = X_t[n_train + n_val:], Y_t[n_train + n_val:]\n",
                "    \n",
                "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
                "    \n",
                "    # Create DataLoaders\n",
                "    train_dataset = TensorDataset(X_train, Y_train)\n",
                "    val_dataset = TensorDataset(X_val, Y_val)\n",
                "    test_dataset = TensorDataset(X_test, Y_test)\n",
                "    \n",
                "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
                "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
                "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
                "    \n",
                "    # Info for visualization later\n",
                "    info = {\n",
                "        't_eval': t_eval,\n",
                "        'x_sim': x_sim,\n",
                "        'forces': forces,\n",
                "        'n_train': n_train,\n",
                "        'n_val': n_val,\n",
                "        'window_size': config.window_size\n",
                "    }\n",
                "    \n",
                "    print(\"✓ Dataset generated successfully!\")\n",
                "    return train_loader, val_loader, test_loader, info\n",
                "\n",
                "\n",
                "# Generate the dataset\n",
                "train_loader, val_loader, test_loader, info = generate_dataset()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 2: RNN Model and Training\n",
                "\n",
                "Now we define and train our Recurrent Neural Network."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.1 RNN Architecture\n",
                "\n",
                "We use a simple **Elman RNN** with:\n",
                "- 2 stacked RNN layers with 64 hidden units each\n",
                "- tanh activation function\n",
                "- A final fully-connected layer to produce the scalar output\n",
                "\n",
                "The network takes a sequence of force values and predicts the resulting position."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DuffingRNN(nn.Module):\n",
                "    \"\"\"\n",
                "    RNN for approximating the Duffing oscillator dynamics.\n",
                "    Uses a simple Elman RNN architecture.\n",
                "    \"\"\"\n",
                "    def __init__(\n",
                "        self,\n",
                "        input_size: int = 1,\n",
                "        hidden_size: int = 64,\n",
                "        num_layers: int = 2,\n",
                "        dropout: float = 0.1\n",
                "    ):\n",
                "        super(DuffingRNN, self).__init__()\n",
                "        \n",
                "        self.hidden_size = hidden_size\n",
                "        self.num_layers = num_layers\n",
                "        \n",
                "        # RNN layers\n",
                "        self.rnn = nn.RNN(\n",
                "            input_size=input_size,\n",
                "            hidden_size=hidden_size,\n",
                "            num_layers=num_layers,\n",
                "            batch_first=True,\n",
                "            dropout=dropout if num_layers > 1 else 0,\n",
                "            nonlinearity='tanh'\n",
                "        )\n",
                "        \n",
                "        # Output layer: maps hidden state to scalar output\n",
                "        self.fc = nn.Linear(hidden_size, 1)\n",
                "        \n",
                "        # Initialize weights for stability\n",
                "        self._init_weights()\n",
                "    \n",
                "    def _init_weights(self):\n",
                "        \"\"\"Orthogonal initialization for better gradient flow\"\"\"\n",
                "        for name, param in self.rnn.named_parameters():\n",
                "            if 'weight_ih' in name:  # Input-hidden weights\n",
                "                nn.init.xavier_uniform_(param)\n",
                "            elif 'weight_hh' in name:  # Hidden-hidden weights\n",
                "                nn.init.orthogonal_(param)\n",
                "            elif 'bias' in name:\n",
                "                nn.init.zeros_(param)\n",
                "    \n",
                "    def forward(self, x: torch.Tensor, h0: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Forward pass.\n",
                "        \n",
                "        Args:\n",
                "            x: Input tensor [batch_size, seq_len, input_size]\n",
                "            h0: Initial hidden state (optional)\n",
                "            \n",
                "        Returns:\n",
                "            Output tensor [batch_size, 1]\n",
                "        \"\"\"\n",
                "        # Pass through RNN layers\n",
                "        out, _ = self.rnn(x, h0)\n",
                "        \n",
                "        # Take only the last time step's output\n",
                "        out = self.fc(out[:, -1, :])\n",
                "        return out\n",
                "\n",
                "\n",
                "# Create model and show summary\n",
                "model = DuffingRNN(\n",
                "    input_size=config.input_size,\n",
                "    hidden_size=config.hidden_size,\n",
                "    num_layers=config.num_layers,\n",
                "    dropout=config.dropout\n",
                ").to(device)\n",
                "\n",
                "n_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\"Model architecture: DuffingRNN\")\n",
                "print(f\"Total parameters: {n_params:,}\")\n",
                "print(model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 Early Stopping\n",
                "\n",
                "Early stopping prevents overfitting by stopping training when validation loss stops improving."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EarlyStopping:\n",
                "    \"\"\"Early stopping to stop training when validation loss stops improving.\"\"\"\n",
                "    \n",
                "    def __init__(self, patience: int = 50, min_delta: float = 1e-6, path: str = \"best_model.pt\"):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            patience: Number of epochs to wait without improvement\n",
                "            min_delta: Minimum change to qualify as improvement\n",
                "            path: Path to save the best model\n",
                "        \"\"\"\n",
                "        self.patience = patience\n",
                "        self.min_delta = min_delta\n",
                "        self.path = path\n",
                "        self.counter = 0\n",
                "        self.best_loss = float('inf')\n",
                "        self.early_stop = False\n",
                "    \n",
                "    def __call__(self, val_loss: float, model: nn.Module):\n",
                "        \"\"\"Check if we should stop and save best model\"\"\"\n",
                "        if val_loss < self.best_loss - self.min_delta:\n",
                "            # Improvement found\n",
                "            self.best_loss = val_loss\n",
                "            self.counter = 0\n",
                "            torch.save(model.state_dict(), self.path)\n",
                "        else:\n",
                "            # No improvement\n",
                "            self.counter += 1\n",
                "            if self.counter >= self.patience:\n",
                "                self.early_stop = True\n",
                "    \n",
                "    def load_best(self, model: nn.Module):\n",
                "        \"\"\"Load the best model weights\"\"\"\n",
                "        model.load_state_dict(torch.load(self.path))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.3 Training Loop\n",
                "\n",
                "Our training includes several best practices:\n",
                "- **Early stopping**: Stop if validation loss doesn't improve\n",
                "- **Learning rate scheduling**: Reduce LR when stuck on a plateau\n",
                "- **Gradient clipping**: Prevent exploding gradients\n",
                "- **Model checkpointing**: Save the best model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model(\n",
                "    train_loader: DataLoader,\n",
                "    val_loader: DataLoader\n",
                ") -> Tuple[nn.Module, dict]:\n",
                "    \"\"\"\n",
                "    Train the RNN model with optimizations.\n",
                "    \n",
                "    Features:\n",
                "    - Early stopping\n",
                "    - Learning rate scheduling (ReduceLROnPlateau)\n",
                "    - Gradient clipping\n",
                "    - Best model saving\n",
                "    \n",
                "    Args:\n",
                "        train_loader: Training DataLoader\n",
                "        val_loader: Validation DataLoader\n",
                "        \n",
                "    Returns:\n",
                "        Tuple (trained_model, training_history)\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"PART 2: TRAINING THE RNN MODEL\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    # Initialize model\n",
                "    model = DuffingRNN(\n",
                "        input_size=config.input_size,\n",
                "        hidden_size=config.hidden_size,\n",
                "        num_layers=config.num_layers,\n",
                "        dropout=config.dropout\n",
                "    ).to(device)\n",
                "    \n",
                "    print(f\"Device: {device}\")\n",
                "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "    \n",
                "    # Loss function and optimizer\n",
                "    criterion = nn.MSELoss()\n",
                "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "    \n",
                "    # Learning rate scheduler: reduce LR when validation loss plateaus\n",
                "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
                "        optimizer, mode='min', factor=0.5, patience=20\n",
                "    )\n",
                "    \n",
                "    # Early stopping\n",
                "    early_stopping = EarlyStopping(\n",
                "        patience=config.patience,\n",
                "        min_delta=config.min_delta,\n",
                "        path=config.checkpoint_path\n",
                "    )\n",
                "    \n",
                "    # Training history\n",
                "    history = {'train_loss': [], 'val_loss': [], 'lr': []}\n",
                "    \n",
                "    print(f\"\\nStarting training: max {config.max_epochs} epochs, patience={config.patience}\")\n",
                "    print(\"-\" * 60)\n",
                "    \n",
                "    for epoch in range(config.max_epochs):\n",
                "        # ===== Training Phase =====\n",
                "        model.train()\n",
                "        train_loss = 0.0\n",
                "        \n",
                "        for X_batch, y_batch in train_loader:\n",
                "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "            \n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(X_batch)\n",
                "            loss = criterion(outputs, y_batch)\n",
                "            loss.backward()\n",
                "            \n",
                "            # Gradient clipping to prevent exploding gradients\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
                "            \n",
                "            optimizer.step()\n",
                "            train_loss += loss.item()\n",
                "        \n",
                "        train_loss /= len(train_loader)\n",
                "        \n",
                "        # ===== Validation Phase =====\n",
                "        model.eval()\n",
                "        val_loss = 0.0\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for X_batch, y_batch in val_loader:\n",
                "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "                outputs = model(X_batch)\n",
                "                loss = criterion(outputs, y_batch)\n",
                "                val_loss += loss.item()\n",
                "        \n",
                "        val_loss /= len(val_loader)\n",
                "        \n",
                "        # Update learning rate scheduler\n",
                "        scheduler.step(val_loss)\n",
                "        \n",
                "        # Record history\n",
                "        history['train_loss'].append(train_loss)\n",
                "        history['val_loss'].append(val_loss)\n",
                "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
                "        \n",
                "        # Check early stopping\n",
                "        early_stopping(val_loss, model)\n",
                "        \n",
                "        # Logging every 20 epochs or at early stopping\n",
                "        if (epoch + 1) % 20 == 0 or early_stopping.early_stop:\n",
                "            print(f\"Epoch [{epoch + 1:4d}/{config.max_epochs}] \"\n",
                "                  f\"Train Loss: {train_loss:.6f} | \"\n",
                "                  f\"Val Loss: {val_loss:.6f} | \"\n",
                "                  f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
                "        \n",
                "        if early_stopping.early_stop:\n",
                "            print(f\"\\n⚠ Early stopping triggered at epoch {epoch + 1}\")\n",
                "            break\n",
                "    \n",
                "    # Load the best model\n",
                "    early_stopping.load_best(model)\n",
                "    print(f\"\\n✓ Best model loaded (val_loss: {early_stopping.best_loss:.6f})\")\n",
                "    \n",
                "    return model, history\n",
                "\n",
                "\n",
                "# Train the model\n",
                "model, history = train_model(train_loader, val_loader)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.4 Visualize Training History"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_training_history(history: dict):\n",
                "    \"\"\"Display training history plots.\"\"\"\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    # Loss curves\n",
                "    ax1 = axes[0]\n",
                "    ax1.plot(history['train_loss'], label='Training Loss', alpha=0.8, linewidth=2)\n",
                "    ax1.plot(history['val_loss'], label='Validation Loss', alpha=0.8, linewidth=2)\n",
                "    ax1.set_xlabel('Epoch', fontsize=12)\n",
                "    ax1.set_ylabel('Loss (MSE)', fontsize=12)\n",
                "    ax1.set_title('Training and Validation Loss', fontsize=14)\n",
                "    ax1.legend(fontsize=11)\n",
                "    ax1.grid(True, alpha=0.3)\n",
                "    ax1.set_yscale('log')\n",
                "    \n",
                "    # Learning rate\n",
                "    ax2 = axes[1]\n",
                "    ax2.plot(history['lr'], 'g-', alpha=0.8, linewidth=2)\n",
                "    ax2.set_xlabel('Epoch', fontsize=12)\n",
                "    ax2.set_ylabel('Learning Rate', fontsize=12)\n",
                "    ax2.set_title('Learning Rate Schedule', fontsize=14)\n",
                "    ax2.grid(True, alpha=0.3)\n",
                "    ax2.set_yscale('log')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(\"training_history.png\", dpi=150)\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"✓ Training history saved: training_history.png\")\n",
                "\n",
                "\n",
                "plot_training_history(history)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 3: Model Evaluation\n",
                "\n",
                "Let's evaluate our trained model on the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_model(\n",
                "    model: nn.Module,\n",
                "    test_loader: DataLoader,\n",
                "    info: dict\n",
                ") -> dict:\n",
                "    \"\"\"\n",
                "    Evaluate the model on the test set.\n",
                "    \n",
                "    Args:\n",
                "        model: Trained model\n",
                "        test_loader: Test DataLoader\n",
                "        info: Dataset info dictionary\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary of metrics\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"PART 3: MODEL EVALUATION\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    model.eval()\n",
                "    \n",
                "    all_preds = []\n",
                "    all_targets = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for X_batch, y_batch in test_loader:\n",
                "            X_batch = X_batch.to(device)\n",
                "            outputs = model(X_batch)\n",
                "            all_preds.append(outputs.cpu().numpy())\n",
                "            all_targets.append(y_batch.numpy())\n",
                "    \n",
                "    predictions = np.concatenate(all_preds, axis=0).flatten()\n",
                "    targets = np.concatenate(all_targets, axis=0).flatten()\n",
                "    \n",
                "    # Calculate metrics\n",
                "    mse = np.mean((predictions - targets) ** 2)\n",
                "    mae = np.mean(np.abs(predictions - targets))\n",
                "    rmse = np.sqrt(mse)\n",
                "    \n",
                "    # R² score (coefficient of determination)\n",
                "    ss_res = np.sum((targets - predictions) ** 2)\n",
                "    ss_tot = np.sum((targets - np.mean(targets)) ** 2)\n",
                "    r2 = 1 - (ss_res / ss_tot)\n",
                "    \n",
                "    metrics = {'MSE': mse, 'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
                "    \n",
                "    print(\"\\nTest Set Metrics:\")\n",
                "    print(f\"  MSE:  {mse:.6f}\")\n",
                "    print(f\"  MAE:  {mae:.6f}\")\n",
                "    print(f\"  RMSE: {rmse:.6f}\")\n",
                "    print(f\"  R²:   {r2:.6f}\")\n",
                "    \n",
                "    # Create visualizations\n",
                "    test_start = info['n_train'] + info['n_val'] + info['window_size']\n",
                "    t_test = info['t_eval'][test_start:test_start + len(predictions)]\n",
                "    x_true = info['x_sim'][test_start:test_start + len(predictions)]\n",
                "    \n",
                "    plt.figure(figsize=(14, 5))\n",
                "    \n",
                "    # Time series comparison\n",
                "    plt.subplot(1, 2, 1)\n",
                "    plt.plot(t_test, x_true, 'b-', label='Physics (Implicit Euler)', alpha=0.7, linewidth=2)\n",
                "    plt.plot(t_test, predictions, 'r--', label='RNN Prediction', alpha=0.7, linewidth=2)\n",
                "    plt.title(\"Comparison: True Dynamics vs RNN Approximation\", fontsize=13)\n",
                "    plt.xlabel(\"Time (s)\", fontsize=11)\n",
                "    plt.ylabel(\"Displacement x(t)\", fontsize=11)\n",
                "    plt.legend(fontsize=10)\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    \n",
                "    # Scatter plot: predictions vs actual\n",
                "    plt.subplot(1, 2, 2)\n",
                "    plt.scatter(x_true, predictions, alpha=0.5, s=10)\n",
                "    plt.plot([x_true.min(), x_true.max()], [x_true.min(), x_true.max()], 'r--', linewidth=2, label='Perfect fit')\n",
                "    plt.title(f\"Predictions vs Reality (R²={r2:.4f})\", fontsize=13)\n",
                "    plt.xlabel(\"True Values\", fontsize=11)\n",
                "    plt.ylabel(\"Predictions\", fontsize=11)\n",
                "    plt.legend(fontsize=10)\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(\"evaluation_results.png\", dpi=150)\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"\\n✓ Evaluation plots saved: evaluation_results.png\")\n",
                "    \n",
                "    return metrics\n",
                "\n",
                "\n",
                "# Evaluate the model\n",
                "metrics = evaluate_model(model, test_loader, info)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 4: Extrapolation Test\n",
                "\n",
                "One of the key questions for any learned model is: **how well does it generalize beyond the training data?**\n",
                "\n",
                "We test this by simulating beyond the original training time horizon and comparing the RNN predictions to the physics-based simulation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def test_extrapolation(model: nn.Module) -> dict:\n",
                "    \"\"\"\n",
                "    Test the model's extrapolation capability on a longer simulation.\n",
                "    \n",
                "    Args:\n",
                "        model: Trained model\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary of extrapolation results\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"PART 4: INFERENCE AND EXTRAPOLATION TEST\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    model.eval()\n",
                "    \n",
                "    # Extended simulation with the same force\n",
                "    print(\"\\n--- Extended Temporal Simulation (same force) ---\")\n",
                "    t_long, x_long, _, forces_long = simulate_duffing(\n",
                "        config.t_max_extrapolation, config.dt, \"standard\"\n",
                "    )\n",
                "    \n",
                "    X_long, Y_long = create_sequences(x_long, forces_long, config.window_size)\n",
                "    X_long_t = torch.tensor(X_long, dtype=torch.float32).unsqueeze(-1).to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        preds_long = model(X_long_t).cpu().numpy().flatten()\n",
                "    \n",
                "    # Compute metrics for \"seen\" (0-100s) vs \"extrapolated\" (100-200s) regions\n",
                "    n_train_equiv = int(config.t_max / config.dt) - config.window_size\n",
                "    \n",
                "    mse_seen = np.mean((preds_long[:n_train_equiv] - Y_long[:n_train_equiv]) ** 2)\n",
                "    mse_extrapolated = np.mean((preds_long[n_train_equiv:] - Y_long[n_train_equiv:]) ** 2)\n",
                "    \n",
                "    print(f\"MSE on 'seen' data (0-100s):       {mse_seen:.6f}\")\n",
                "    print(f\"MSE on extrapolation (100-200s):   {mse_extrapolated:.6f}\")\n",
                "    print(f\"Degradation ratio: {mse_extrapolated / mse_seen:.2f}x\")\n",
                "    \n",
                "    # Visualization\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    # Full time series\n",
                "    ax1 = axes[0]\n",
                "    t_plot = t_long[config.window_size:]\n",
                "    ax1.plot(t_plot, Y_long, 'b-', label='Physics', alpha=0.7, linewidth=1.5)\n",
                "    ax1.plot(t_plot, preds_long, 'r--', label='RNN', alpha=0.7, linewidth=1.5)\n",
                "    ax1.axvline(x=config.t_max, color='g', linestyle=':', linewidth=2, label='End of training data')\n",
                "    ax1.set_title(\"Temporal Extrapolation (0-200s)\", fontsize=13)\n",
                "    ax1.set_xlabel(\"Time (s)\", fontsize=11)\n",
                "    ax1.set_ylabel(\"x(t)\", fontsize=11)\n",
                "    ax1.legend(fontsize=10)\n",
                "    ax1.grid(True, alpha=0.3)\n",
                "    \n",
                "    # Zoom on extrapolation region\n",
                "    ax2 = axes[1]\n",
                "    mask = t_plot > config.t_max\n",
                "    ax2.plot(t_plot[mask], Y_long[mask], 'b-', label='Physics', alpha=0.7, linewidth=1.5)\n",
                "    ax2.plot(t_plot[mask], preds_long[mask], 'r--', label='RNN', alpha=0.7, linewidth=1.5)\n",
                "    ax2.set_title(\"Zoom on Extrapolation Region (100-200s)\", fontsize=13)\n",
                "    ax2.set_xlabel(\"Time (s)\", fontsize=11)\n",
                "    ax2.set_ylabel(\"x(t)\", fontsize=11)\n",
                "    ax2.legend(fontsize=10)\n",
                "    ax2.grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(\"extrapolation_results.png\", dpi=150)\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"\\n✓ Extrapolation plots saved: extrapolation_results.png\")\n",
                "    \n",
                "    results = {\n",
                "        'mse_seen': mse_seen,\n",
                "        'mse_extrapolated': mse_extrapolated,\n",
                "        'degradation_ratio': mse_extrapolated / mse_seen\n",
                "    }\n",
                "    \n",
                "    return results\n",
                "\n",
                "\n",
                "# Test extrapolation\n",
                "extrapolation_results = test_extrapolation(model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Summary\n",
                "\n",
                "Let's summarize our results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"                    FINAL SUMMARY\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "print(\"\\n📊 Test Set Metrics:\")\n",
                "print(f\"   • MSE:  {metrics['MSE']:.6f}\")\n",
                "print(f\"   • RMSE: {metrics['RMSE']:.6f}\")\n",
                "print(f\"   • MAE:  {metrics['MAE']:.6f}\")\n",
                "print(f\"   • R²:   {metrics['R2']:.6f}\")\n",
                "\n",
                "print(\"\\n🔮 Extrapolation Capability:\")\n",
                "print(f\"   • MSE on seen data (0-100s):     {extrapolation_results['mse_seen']:.6f}\")\n",
                "print(f\"   • MSE on extrapolation (100-200s): {extrapolation_results['mse_extrapolated']:.6f}\")\n",
                "print(f\"   • Degradation ratio: {extrapolation_results['degradation_ratio']:.2f}x\")\n",
                "\n",
                "print(\"\\n✅ Pipeline complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Conclusions\n",
                "\n",
                "In this notebook, we have:\n",
                "\n",
                "1. **Simulated a nonlinear dynamical system** (Duffing oscillator) using implicit Euler integration\n",
                "2. **Trained an RNN** to predict the system's response from a sequence of force inputs\n",
                "3. **Evaluated the model** on held-out test data\n",
                "4. **Tested extrapolation** beyond the training time horizon\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "- RNNs can effectively learn to approximate nonlinear dynamics when trained on sufficient data\n",
                "- The sliding window approach allows the network to learn temporal dependencies\n",
                "- Extrapolation performance depends on how similar the new conditions are to training conditions\n",
                "- Implicit numerical schemes (like implicit Euler) provide stable reference solutions for generating training data\n",
                "\n",
                "### Extensions\n",
                "\n",
                "You could extend this work by:\n",
                "- Trying different architectures (LSTM, GRU, Transformer)\n",
                "- Adding physics-informed constraints to the loss function\n",
                "- Testing with more complex forcing functions\n",
                "- Exploring different hyperparameters"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}