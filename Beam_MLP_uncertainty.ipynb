{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Ensemble + Uncertainty + SHAP Analysis\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Dataset Generation** - Creating synthetic nonlinear beam deflection data\n",
    "2. **Surrogate Modeling** - Training a deep ensemble with uncertainty quantification\n",
    "3. **Sensitivity Analysis** - Using SHAP to understand feature importance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import shap\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"SHAP version: {shap.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Data configuration\n",
    "FEATURE_NAMES = [\"P_load_N\", \"E_Pa\", \"h_m\"]\n",
    "TARGET_NAME = \"delta_m\"\n",
    "\n",
    "# Dataset generation\n",
    "N_SAMPLES = 200\n",
    "NOISE_REL = 0.01\n",
    "\n",
    "# Train/test splits\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "# Ensemble configuration\n",
    "M_ENSEMBLE = 10\n",
    "EPOCHS = 2000\n",
    "BATCH_SIZE = 32\n",
    "LR = 2e-3\n",
    "WEIGHT_DECAY = 2e-4\n",
    "PATIENCE = 200\n",
    "\n",
    "# SHAP configuration\n",
    "MAX_EXPLAIN = 120\n",
    "SHAP_BACKGROUND = 50\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Random seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 1: Dataset Generation\n",
    "\n",
    "We generate synthetic data for a **nonlinear beam deflection problem**.\n",
    "\n",
    "**Physics Model:**\n",
    "- A simply supported beam under central load P\n",
    "- Nonlinear stiffness: $k_3\\delta^3 + k_1\\delta - P = 0$\n",
    "\n",
    "**Input Features:**\n",
    "- `P`: Load (N)\n",
    "- `E`: Young's modulus (Pa)\n",
    "- `h`: Beam height (m)\n",
    "\n",
    "**Output:**\n",
    "- `delta`: Beam deflection (m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BeamParams:\n",
    "    \"\"\"Parameters for a strongly nonlinear beam model.\"\"\"\n",
    "    L: float = 1.0\n",
    "    b: float = 0.05\n",
    "\n",
    "    # Strong nonlinearity controls\n",
    "    k3_factor: float = 0.50\n",
    "    k5_factor: float = 5.0\n",
    "\n",
    "    # Solver params\n",
    "    tol: float = 1e-10\n",
    "    max_iter: int = 200\n",
    "    max_delta: float = 0.5\n",
    "\n",
    "\n",
    "def solve_beam_deflection(P, E, h, params: BeamParams) -> float:\n",
    "    \"\"\"\n",
    "    Solve: k5*δ^5 + k3*δ^3 + k1*δ - P = 0  for δ >= 0\n",
    "    Strongly nonlinear (hardening) model. Robust bisection bracketing.\n",
    "    \"\"\"\n",
    "    L, b = params.L, params.b\n",
    "    A = b * h\n",
    "    I = b * h**3 / 12.0\n",
    "\n",
    "    # Linear stiffness proxy (simply supported beam, central load)\n",
    "    k1 = 48.0 * E * I / (L**3)  # [N/m]\n",
    "\n",
    "    # Thickness-sensitive nonlinearities (strong when h is small)\n",
    "    base = (E * A) / (L**3)     # [N/m^2]\n",
    "    k3 = params.k3_factor * base / (h**2)  # [N/m^3]\n",
    "    k5 = params.k5_factor * base / (h**4)  # [N/m^5]\n",
    "\n",
    "    def f(d):\n",
    "        return (k5 * d**5) + (k3 * d**3) + (k1 * d) - P\n",
    "\n",
    "    # Bracket\n",
    "    lo = 0.0\n",
    "    hi = 1e-9\n",
    "    fhi = f(hi)\n",
    "    while fhi <= 0.0 and hi < params.max_delta:\n",
    "        hi *= 2.0\n",
    "        fhi = f(hi)\n",
    "\n",
    "    if fhi <= 0.0:\n",
    "        return float(params.max_delta)\n",
    "\n",
    "    # Bisection\n",
    "    for _ in range(params.max_iter):\n",
    "        mid = 0.5 * (lo + hi)\n",
    "        fmid = f(mid)\n",
    "        if abs(fmid) < params.tol or (hi - lo) < params.tol:\n",
    "            return float(mid)\n",
    "        if fmid > 0:\n",
    "            hi = mid\n",
    "        else:\n",
    "            lo = mid\n",
    "\n",
    "    return float(0.5 * (lo + hi))\n",
    "\n",
    "\n",
    "def generate_dataset(n: int, seed: int, params: BeamParams):\n",
    "    \"\"\"\n",
    "    Small dataset (few simulations), WITHOUT noise.\n",
    "    Inputs: P (N), E (Pa), h (m). Output: delta (m)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    P = rng.uniform(50.0, 8000.0, size=n)\n",
    "    E = rng.uniform(70e9, 210e9, size=n)\n",
    "    h = rng.uniform(0.003, 0.04, size=n)\n",
    "\n",
    "    delta = np.zeros(n, dtype=float)\n",
    "    for i in range(n):\n",
    "        delta[i] = solve_beam_deflection(P[i], E[i], h[i], params)\n",
    "\n",
    "    y = delta.reshape(-1, 1)\n",
    "    X = np.stack([P, E, h], axis=1)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_nonlinearity_cases(params: BeamParams, n_cases: int = 4, n_points: int = 150):\n",
    "    \"\"\"\n",
    "    Plot δ(P) for a few fixed (E,h) cases to illustrate strong nonlinearity.\n",
    "    \"\"\"\n",
    "    # Pick representative cases (you can change these to match your domain)\n",
    "    cases = [\n",
    "        {\"E\": 70e9,  \"h\": 0.035},\n",
    "        {\"E\": 70e9,  \"h\": 0.006},\n",
    "        {\"E\": 210e9, \"h\": 0.035},\n",
    "        {\"E\": 210e9, \"h\": 0.006},\n",
    "    ][:n_cases]\n",
    "\n",
    "    P_grid = np.linspace(0.0, 8000.0, n_points)\n",
    "\n",
    "    plt.figure()\n",
    "    for c in cases:\n",
    "        E, h = c[\"E\"], c[\"h\"]\n",
    "        deltas = np.array([solve_beam_deflection(P, E, h, params) for P in P_grid])\n",
    "        label = f\"E={E/1e9:.0f} GPa, h={h*1e3:.1f} mm\"\n",
    "        plt.plot(P_grid, deltas, label=label)\n",
    "\n",
    "    plt.xlabel(\"Load P (N)\")\n",
    "    plt.ylabel(\"Mid-span deflection δ (m)\")\n",
    "    plt.title(\"Strong nonlinearity illustration: δ(P) for fixed (E,h)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage\n",
    "# ----------------------------\n",
    "\n",
    "# Hardcoded example config (adapt to your variables)\n",
    "N_SAMPLES = 200\n",
    "SEED = 42\n",
    "FEATURE_NAMES = [\"P_load_N\", \"E_Pa\", \"h_m\"]\n",
    "TARGET_NAME = \"delta_m\"\n",
    "\n",
    "params = BeamParams(\n",
    "    L=1.0,\n",
    "    b=0.05,\n",
    "    k3_factor=0.50,\n",
    "    k5_factor=5.0,\n",
    ")\n",
    "\n",
    "X, y = generate_dataset(N_SAMPLES, SEED, params)\n",
    "\n",
    "print(f\"Generated {N_SAMPLES} samples\")\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "print(\"Noise level: 0% (deterministic)\")\n",
    "\n",
    "df = pd.DataFrame(X, columns=FEATURE_NAMES)\n",
    "df[TARGET_NAME] = y\n",
    "\n",
    "# Plot a few deterministic curves to show nonlinearity\n",
    "plot_nonlinearity_cases(params, n_cases=4, n_points=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate & Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "params = BeamParams()\n",
    "X, y = generate_dataset(N_SAMPLES, SEED, params, noise_rel=NOISE_REL)\n",
    "\n",
    "print(f\"Generated {N_SAMPLES} samples\")\n",
    "print(f\"Features: {FEATURE_NAMES}\")\n",
    "print(f\"Target: {TARGET_NAME}\")\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "print(f\"Noise level: {NOISE_REL*100}%\")\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "df = pd.DataFrame(X, columns=FEATURE_NAMES)\n",
    "df[TARGET_NAME] = y\n",
    "\n",
    "print(\"\\nDataset statistics:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize input distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for i, col in enumerate(FEATURE_NAMES):\n",
    "    ax = axes[i//2, i%2]\n",
    "    ax.hist(df[col], bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax.set_title(f'Distribution of {col}', fontsize=12)\n",
    "    ax.set_xlabel(col, fontsize=10)\n",
    "    ax.set_ylabel('Frequency', fontsize=10)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Target distribution\n",
    "ax = axes[1, 1]\n",
    "ax.hist(df[TARGET_NAME], bins=30, alpha=0.7, color='red', edgecolor='black')\n",
    "ax.set_title(f'Distribution of {TARGET_NAME}', fontsize=12)\n",
    "ax.set_xlabel(TARGET_NAME, fontsize=10)\n",
    "ax.set_ylabel('Frequency', fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split & Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, random_state=SEED)\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X_tr, y_tr, test_size=VAL_SIZE, random_state=SEED)\n",
    "\n",
    "print(f\"Train: {len(X_tr)}, Val: {len(X_va)}, Test: {len(X_te)}\")\n",
    "\n",
    "# Scale data\n",
    "x_scaler = StandardScaler().fit(X_tr)\n",
    "y_scaler = StandardScaler().fit(y_tr)\n",
    "\n",
    "X_tr_s = x_scaler.transform(X_tr)\n",
    "X_va_s = x_scaler.transform(X_va)\n",
    "X_te_s = x_scaler.transform(X_te)\n",
    "\n",
    "y_tr_s = y_scaler.transform(y_tr)\n",
    "y_va_s = y_scaler.transform(y_va)\n",
    "y_te_s = y_scaler.transform(y_te)\n",
    "\n",
    "print(\"✓ Data split and scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 2: Surrogate Modeling with Deep Ensemble\n",
    "\n",
    "Train an **ensemble of neural networks** to predict beam deflection.\n",
    "\n",
    "**Key features:**\n",
    "- Bootstrap sampling for diversity\n",
    "- Early stopping to prevent overfitting\n",
    "- **Epistemic uncertainty** quantification via ensemble variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressor(nn.Module):\n",
    "    \"\"\"Multi-layer perceptron for regression.\"\"\"\n",
    "    def __init__(self, in_dim=3, hidden=(64, 64), dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Test model creation\n",
    "test_model = MLPRegressor(in_dim=3, hidden=(64, 64))\n",
    "print(f\"Model architecture:\\n{test_model}\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in test_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_model(\n",
    "    X_train_s, y_train_s,\n",
    "    X_val_s, y_val_s,\n",
    "    seed: int,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    patience: int,\n",
    "    device: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a single MLP model with early stopping.\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained PyTorch model\n",
    "        best_val: Best validation loss\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = MLPRegressor(in_dim=X_train_s.shape[1], hidden=(64, 64), dropout=0.0).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    train_ds = TensorDataset(\n",
    "        torch.tensor(X_train_s, dtype=torch.float32),\n",
    "        torch.tensor(y_train_s, dtype=torch.float32),\n",
    "    )\n",
    "    loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    val_x = torch.tensor(X_val_s, dtype=torch.float32).to(device)\n",
    "    val_y = torch.tensor(y_val_s, dtype=torch.float32).to(device)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vpred = model(val_x)\n",
    "            vloss = loss_fn(vpred, val_y).item()\n",
    "\n",
    "        if vloss < best_val - 1e-7:\n",
    "            best_val = vloss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, best_val\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ensemble_predict_scaled(models, X_s: np.ndarray, device: str):\n",
    "    \"\"\"\n",
    "    Predicts in normalized space.\n",
    "    \n",
    "    Returns:\n",
    "      mean_s: (n,1) - mean prediction\n",
    "      var_s : (n,1) - variance inter-models (epistemic uncertainty proxy)\n",
    "      all_s : (M,n,1) - all individual predictions\n",
    "    \"\"\"\n",
    "    x = torch.tensor(X_s, dtype=torch.float32).to(device)\n",
    "    preds = []\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "        preds.append(m(x).detach().cpu().numpy())\n",
    "    all_preds = np.stack(preds, axis=0)\n",
    "    mean = np.mean(all_preds, axis=0)\n",
    "    var = np.var(all_preds, axis=0, ddof=1)\n",
    "    return mean, var, all_preds\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble with bootstrap\n",
    "models = []\n",
    "val_losses = []\n",
    "n_train = len(X_tr_s)\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "print(f\"Training {M_ENSEMBLE} models with bootstrap...\")\n",
    "print(f\"Device: {DEVICE}\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "for m in range(M_ENSEMBLE):\n",
    "    # Bootstrap sampling\n",
    "    idx = rng.integers(low=0, high=n_train, size=n_train)\n",
    "    Xb = X_tr_s[idx]\n",
    "    yb = y_tr_s[idx]\n",
    "\n",
    "    seed_m = SEED + 1000 + m * 17\n",
    "    model, vloss = train_one_model(\n",
    "        Xb, yb, X_va_s, y_va_s,\n",
    "        seed=seed_m,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=min(BATCH_SIZE, max(8, n_train)),\n",
    "        lr=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        patience=PATIENCE,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    models.append(model)\n",
    "    val_losses.append(vloss)\n",
    "    print(f\"  Model {m+1}/{M_ENSEMBLE}: val_loss = {vloss:.6f}\")\n",
    "\n",
    "train_time = time.time() - t0\n",
    "print(f\"\\n✓ Ensemble training complete in {train_time:.2f}s\")\n",
    "print(f\"Average validation loss: {np.mean(val_losses):.6f} ± {np.std(val_losses):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "mean_s, var_s, all_s = ensemble_predict_scaled(models, X_te_s, device=DEVICE)\n",
    "y_pred = y_scaler.inverse_transform(mean_s)\n",
    "\n",
    "# Epistemic variance in original units: var_orig = var_scaled * y_scale^2\n",
    "y_scale = float(y_scaler.scale_[0])\n",
    "y_var_epistemic = var_s * (y_scale ** 2)\n",
    "\n",
    "# Calculate metrics\n",
    "pred_mean = float(np.mean(y_pred))\n",
    "pred_var = float(np.var(y_pred))\n",
    "epistemic_mean_var = float(np.mean(y_var_epistemic))\n",
    "epistemic_var_var = float(np.var(y_var_epistemic))\n",
    "\n",
    "print(\"\\n=== METRICS ===\")\n",
    "print(f\"Prediction mean:                {pred_mean:.6e}\")\n",
    "print(f\"Prediction variance:            {pred_var:.6e}\")\n",
    "print(f\"Epistemic uncertainty mean:     {epistemic_mean_var:.6e}\")\n",
    "print(f\"Epistemic uncertainty variance: {epistemic_var_var:.6e}\")\n",
    "print(f\"Training time:                  {train_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs True\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(y_te, y_pred, s=30, alpha=0.6, edgecolors='k', linewidths=0.5)\n",
    "mn = min(y_te.min(), y_pred.min())\n",
    "mx = max(y_te.max(), y_pred.max())\n",
    "ax.plot([mn, mx], [mn, mx], 'r--', lw=2, label='Perfect prediction')\n",
    "ax.set_xlabel(\"True δ (m)\", fontsize=12)\n",
    "ax.set_ylabel(\"Predicted δ (m)\", fontsize=12)\n",
    "ax.set_title(\"Ensemble Mean vs True (Test Set)\", fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty intervals\n",
    "idx = np.argsort(y_te.ravel())\n",
    "yt = y_te.ravel()[idx]\n",
    "ym = y_pred.ravel()[idx]\n",
    "yv = y_var_epistemic.ravel()[idx]\n",
    "ys = np.sqrt(yv)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "x = np.arange(len(yt))\n",
    "ax.plot(x, yt, label=\"True\", lw=2, color='black')\n",
    "ax.plot(x, ym, label=\"Ensemble mean\", lw=2, color='blue')\n",
    "ax.fill_between(x, ym - 2 * ys, ym + 2 * ys, alpha=0.3, color='blue', label=\"±2σ (epistemic)\")\n",
    "ax.set_xlabel(\"Test samples (sorted by true δ)\", fontsize=12)\n",
    "ax.set_ylabel(\"δ (m)\", fontsize=12)\n",
    "ax.set_title(\"Epistemic Uncertainty (Ensemble Spread)\", fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 3: Sensitivity Analysis with SHAP\n",
    "\n",
    "Use **SHAP (SHapley Additive exPlanations)** to understand:\n",
    "- Which features are most important?\n",
    "- How do features influence predictions?\n",
    "- Model interpretability with uncertainty quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleWrapper:\n",
    "    \"\"\"\n",
    "    Callable for SHAP:\n",
    "      input: X in original units\n",
    "      output: y_pred_mean in original units (shape (n,))\n",
    "    \"\"\"\n",
    "    def __init__(self, models, x_scaler: StandardScaler, y_scaler: StandardScaler, device: str):\n",
    "        self.models = models\n",
    "        self.x_scaler = x_scaler\n",
    "        self.y_scaler = y_scaler\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        X_s = self.x_scaler.transform(X)\n",
    "        mean_s, _, _ = ensemble_predict_scaled(self.models, X_s, self.device)\n",
    "        mean = self.y_scaler.inverse_transform(mean_s).ravel()\n",
    "        return mean\n",
    "\n",
    "# Create wrapper\n",
    "wrapper = EnsembleWrapper(models, x_scaler, y_scaler, device=DEVICE)\n",
    "print(\"✓ Ensemble wrapper created for SHAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample background data\n",
    "bg = shap.sample(X_tr, min(SHAP_BACKGROUND, len(X_tr)), random_state=0)\n",
    "print(f\"Background samples: {len(bg)}\")\n",
    "\n",
    "# Create explainer\n",
    "explainer = shap.KernelExplainer(wrapper, bg)\n",
    "\n",
    "# Select samples to explain\n",
    "X_explain = X_te if len(X_te) <= MAX_EXPLAIN else shap.sample(X_te, MAX_EXPLAIN, random_state=1)\n",
    "print(f\"Explaining {len(X_explain)} test samples...\")\n",
    "\n",
    "# Compute SHAP values (this may take a few minutes)\n",
    "shap_values = explainer.shap_values(X_explain, nsamples=\"auto\")\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[0]\n",
    "\n",
    "print(f\"✓ SHAP values computed: {shap_values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Summary Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot (feature importance + distribution)\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_explain, feature_names=FEATURE_NAMES, show=False)\n",
    "plt.title(\"SHAP Summary Plot - Feature Impact on Predictions\", fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot with variance (global feature importance)\n",
    "# Calculate mean and variance of absolute SHAP values for each feature\n",
    "shap_abs = np.abs(shap_values)\n",
    "feature_importance_mean = np.mean(shap_abs, axis=0)\n",
    "feature_importance_std = np.std(shap_abs, axis=0)\n",
    "feature_importance_var = np.var(shap_abs, axis=0)\n",
    "\n",
    "# Sort by mean importance\n",
    "sorted_idx = np.argsort(feature_importance_mean)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "y_pos = np.arange(len(FEATURE_NAMES))\n",
    "\n",
    "ax.barh(y_pos, feature_importance_mean[sorted_idx], \n",
    "        xerr=feature_importance_std[sorted_idx], \n",
    "        align=\"center\", alpha=0.7, color=\"steelblue\",\n",
    "        error_kw={\"elinewidth\": 2, \"capsize\": 5, \"capthick\": 2})\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([FEATURE_NAMES[i] for i in sorted_idx])\n",
    "ax.set_xlabel(\"mean(|SHAP value|)\", fontsize=12)\n",
    "ax.set_title(\"SHAP Feature Importance (Mean ± Std)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Feature Importance Statistics:\")\n",
    "print(\"=\"*50)\n",
    "for i, fname in enumerate(FEATURE_NAMES):\n",
    "    print(f\"{fname:15s}: mean={feature_importance_mean[i]:.6f}, \"\n",
    "          f\"std={feature_importance_std[i]:.6f}, var={feature_importance_var[i]:.6e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Dependence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence plots for each feature\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, feature_name in enumerate(FEATURE_NAMES):\n",
    "    plt.sca(axes[i])\n",
    "    shap.dependence_plot(i, shap_values, X_explain, feature_names=FEATURE_NAMES, show=False)\n",
    "    axes[i].set_title(f\"SHAP Dependence - {feature_name}\", fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Dataset Generation** ✓\n",
    "   - Created synthetic nonlinear beam deflection data\n",
    "   - Visualized input/output distributions\n",
    "\n",
    "2. **Surrogate Modeling** ✓\n",
    "   - Trained deep ensemble (10 MLPs) with bootstrap\n",
    "   - Quantified epistemic uncertainty (ensemble variance)\n",
    "   - Visualized predictions with confidence intervals\n",
    "\n",
    "3. **Sensitivity Analysis** ✓\n",
    "   - Used SHAP to explain model predictions\n",
    "   - Identified important features and their impacts\n",
    "   - Visualized feature dependencies\n",
    "\n",
    "**Key Metrics:**\n",
    "- Prediction mean & variance\n",
    "- Epistemic uncertainty (mean & variance)\n",
    "- SHAP-based feature importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
